import time

import gym
import numpy as np
import math
from gym import spaces
from irsim.env import EnvBase
from stable_baselines3.common.evaluation import evaluate_policy
import torch
import torch.nn as nn
import torch.nn.functional as F
from stable_baselines3 import SAC,PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CallbackList
from stable_baselines3.common.callbacks import EvalCallback
import os
from collections import deque
import random

class IRSIMEnv(gym.Env):     #ç»§æ‰¿äº† gym.Env
    def __init__(self, config_path='./easy.yaml', display=False):
        super(IRSIMEnv, self).__init__()
        self.env = EnvBase(config_path, save_ani=False, full=False, display=display)    #åŠ è½½åº•å±‚ä»¿çœŸç¯å¢ƒ EnvBaseï¼Œè¯¥ç±»è´Ÿè´£å®é™…çš„æœºå™¨äººå»ºæ¨¡ã€åœ°å›¾ã€éšœç¢ç‰©ç­‰ã€‚

        # åŠ¨ä½œç©ºé—´ä¸ºçº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦ï¼ŒèŒƒå›´ [-1, 1]
        self.action_space = spaces.Box(low=np.array([-1.0, -1.0]), high=np.array([1.0, 1.0]), dtype=np.float32)     #äºŒç»´è¿ç»­åŠ¨ä½œç©ºé—´å®šä¹‰

        # å‚æ•°è®¾ç½®
        self.max_linear_vel = 2.0   # æœ€å¤§çº¿é€Ÿåº¦ m/s
        self.max_steering_angle = 0.523  # æœ€å¤§è§’é€Ÿåº¦ rad/s

        self.velocity = 0.0
        self.steering_angle = 0.0

        self.lidar_max_range = 8.0      # æ¿€å…‰æœ€å¤§æ¢æµ‹èŒƒå›´
        self.goal_range = 14.0          # ç›®æ ‡å¯èƒ½å‡ºç°åœ¨çš„æœ€å¤§èŒƒå›´
        self.robot_radius = 1.0         # æœºå™¨äººåŠå¾„
        self.field_size = 14.0          # åœºåœ°å°ºå¯¸ï¼ˆç”¨äºå½’ä¸€åŒ–ç­‰ï¼‰
        self.max_steps = 300            # æ¯è½®æœ€å¤§æ­¥æ•°
        self.max_goal_steps = 150       # æ¯ä¸ªç›®æ ‡æœ€å¤§å°è¯•æ­¥æ•°
        self.step_count = 0             # æ­¥æ•°è®¡æ•°å™¨
        self.goal_count = 0             # æˆåŠŸè¾¾åˆ°ç›®æ ‡æ¬¡æ•°
        self.prev_distance = None       # ä¸Šä¸€æ­¥åˆ°ç›®æ ‡çš„è·ç¦»


        self.obstacles = np.array([])       # éšœç¢ç‰©åˆ—è¡¨
        self.obstacle_radius = 1.5          # éšœç¢ç‰©ç¢°æ’åˆ¤å®šåŠå¾„
        self.goal_radius = 1.0              # åˆ°è¾¾ç›®æ ‡çš„åˆ¤å®šåŠå¾„


        # è§‚æµ‹åºåˆ—é•¿åº¦
        self.history_len = 5            # ä½¿ç”¨è¿‡å»5å¸§è§‚æµ‹ä½œä¸ºè¾“å…¥
        self.obs_history = deque(maxlen=self.history_len)
        self.prev_min_distance = 999    # åˆå§‹åŒ–æœ€å°è·ç¦»
        # å…ˆè·å–å•å¸§è§‚æµ‹é•¿åº¦   ğŸ“ çŠ¶æ€ç©ºé—´å®šä¹‰
        dummy_obs = self._get_obs()     # è·å–ä¸€å¸§åˆå§‹è§‚æµ‹æ•°æ®
        obs_dim = len(dummy_obs)        # å•å¸§è§‚æµ‹é•¿åº¦

        # ä¿®æ”¹ä¸ºåºåˆ—ç©ºé—´ï¼Œshape=(history_len, obs_dim)
        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(self.history_len, obs_dim), dtype=np.float32)

    def sample_goal(self):       ##ç”¨æ¥éšæœºé‡‡æ ·ä¸€ä¸ªç›®æ ‡ç‚¹çš„ä½ç½®ã€‚  æ£€æµ‹æ˜¯å¦ä¸ºæœ‰æ•ˆç‚¹
        while True:       #ä½¿ç”¨ä¸€ä¸ªæ— é™å¾ªç¯ï¼Œç›´åˆ°é‡‡æ ·å‡ºä¸€ä¸ªåˆæ³•çš„ç›®æ ‡ä¸ºæ­¢ï¼ˆåˆæ³•æŒ‡ï¼šä¸ä¸éšœç¢ç‰©å†²çªï¼‰ã€‚    åœ¨åœºåœ°èŒƒå›´å†…éšæœºç”Ÿæˆä¸€ä¸ª x, y åæ ‡ï¼š
            x = np.random.uniform(self.goal_radius, self.field_size - self.goal_radius)
            y = np.random.uniform(self.field_size - self.goal_radius-1, self.field_size - self.goal_radius)
            goal = np.array([x, y])    #æŠŠé‡‡æ ·ç»“æœæ‰“åŒ…æˆåæ ‡ç‚¹ã€‚
            if self.obstacles.size != 0:    #å¦‚æœå½“å‰ç¯å¢ƒä¸­æœ‰éšœç¢ç‰©ï¼ˆéšœç¢ç‰©ä¸æ˜¯ç©ºï¼‰ï¼Œå°±è¦æ£€æŸ¥ç›®æ ‡æ˜¯å¦ç¦»éšœç¢å¤ªè¿‘ã€‚
                dists = np.linalg.norm(self.obstacles - goal, axis=1)
                if np.all(dists >= (self.obstacle_radius + self.goal_radius)):
                    return np.array([x, y, 0, 0])     #å¦‚æœæ²¡æœ‰éšœç¢ç‰©ï¼Œé‚£ç›´æ¥è¿”å›å³å¯ï¼Œä¸ç”¨æ£€æŸ¥å†²çªã€‚
            else:
                return np.array([x, y, 0, 0])

    def _target_relative_pose(self):     #è®¡ç®—ç›®æ ‡ç‚¹ç›¸å¯¹äºæœºå™¨äººå½“å‰ä½ç½®çš„ç›¸å¯¹ä½ç½®ï¼ˆdx, dyï¼‰å’Œç›¸å¯¹æœå‘å·®ï¼ˆdÎ¸ï¼‰ï¼Œå³æŠŠç›®æ ‡çš„ä½ç½®è½¬æ¢ä¸ºä»¥æœºå™¨äººè‡ªèº«ä¸ºåæ ‡åŸç‚¹å’Œæœå‘åŸºå‡†çš„å±€éƒ¨åæ ‡ç³»ä¸‹çš„ä½ç½®å’Œæ–¹å‘ã€‚
        robot_state = self.env.get_robot_state()
        goal = self.env.robot._goal[0]
        px, py, pt = robot_state[0][0], robot_state[1][0], robot_state[2][0]
        tx, ty, tt = goal[0], goal[1], goal[2]

        dx, dy = tx - px, ty - py
        x_rel = np.cos(pt) * dx + np.sin(pt) * dy
        y_rel = -np.sin(pt) * dx + np.cos(pt) * dy
        theta_rel = (tt - pt + np.pi) % (2 * np.pi) - np.pi
        return [x_rel, y_rel, theta_rel]

    def _get_obs(self):        #è·å–å½“å‰ç¯å¢ƒçš„è§‚æµ‹ï¼ˆobservationï¼‰ä¿¡æ¯
        lidar = np.array(self.env.get_lidar_scan()["ranges"], dtype=np.float32)    # ä»ä»¿çœŸç¯å¢ƒä¸­è·å–æ¿€å…‰é›·è¾¾æ‰«æç»“æœï¼ˆä¸€ä¸ª listï¼ŒåŒ…å«180ä¸ªæ–¹å‘çš„è·ç¦»ä¿¡æ¯ï¼‰ï¼Œè½¬ä¸º np.array
        lidar = np.clip(lidar, 0.0, self.lidar_max_range)                          # æŠŠæ¿€å…‰æ•°æ®é™åˆ¶åœ¨ [0, lidar_max_range] èŒƒå›´å†…ï¼Œè¶…å‡ºå°±è£å‰ªã€‚
        lidar = (lidar / self.lidar_max_range) * 2.0 - 1.0  # [-1, 1]              # å°†è·ç¦»æ•°æ®å½’ä¸€åŒ–åˆ° [-1, 1]ï¼Œæ–¹ä¾¿ç¥ç»ç½‘ç»œè®­ç»ƒã€‚

        target_pose = np.array(self._target_relative_pose(), dtype=np.float32)     #è·å–æœºå™¨äººåˆ°ç›®æ ‡ç‚¹çš„ç›¸å¯¹ä½å§¿ï¼šé€šå¸¸æ˜¯ [dx, dy, dÎ¸]ï¼Œå•ä½å¯èƒ½æ˜¯ç±³å’Œå¼§åº¦ã€‚
        target_pose[0] = np.clip(target_pose[0], -self.goal_range, self.goal_range) / self.goal_range     #å°†ç›¸å¯¹ä½ç½®çš„ dx, dy é™åˆ¶åœ¨æœ€å¤§èŒƒå›´å†…ï¼Œå¹¶å½’ä¸€åŒ–åˆ° [-1, 1]ã€‚
        target_pose[1] = np.clip(target_pose[1], -self.goal_range, self.goal_range) / self.goal_range
        target_pose[2] = np.clip(target_pose[2], -np.pi, np.pi) / np.pi                                   #å°†è§’åº¦å·®ï¼ˆæ–¹å‘å·®ï¼‰è£å‰ªåœ¨ [-Ï€, Ï€] ä¹‹é—´ï¼Œå¹¶å½’ä¸€åŒ–åˆ° [-1, 1]ã€‚
        target_pose = np.clip(target_pose, -1.0, 1.0)                                                     #target_pose = np.clip(target_pose, -1.0, 1.0)
               
        velocity_norm = np.clip(self.velocity, -self.max_linear_vel, self.max_linear_vel) / self.max_linear_vel        #å°†å½“å‰çº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦é™åˆ¶åœ¨èŒƒå›´å†…å¹¶å½’ä¸€åŒ–åˆ° [-1, 1]ã€‚
        angular_norm = np.clip(self.steering_angle, -self.max_steering_angle, self.max_steering_angle) / self.max_steering_angle
        vehicle_info = np.array([velocity_norm, angular_norm], dtype=np.float32)            #æ‰“åŒ…æˆ [çº¿é€Ÿåº¦, è§’é€Ÿåº¦] çš„æ•°ç»„ã€‚

        return np.concatenate([lidar, target_pose, vehicle_info], dtype=np.float32)          #æŠŠä¸‰ä¸ªéƒ¨åˆ†åˆæˆä¸€ä¸ªå®Œæ•´çš„è§‚æµ‹å‘é‡ï¼š( 180+3+2 ) * 5    ä¸€ä¸ª (5, 185) çš„å¼ é‡

    def _get_obs_sequence(self):    #è¿”å›ä¸€æ®µæ—¶é—´åºåˆ—è§‚æµ‹ï¼ˆå³è¿‡å»è¿ç»­å‡ å¸§çš„è§‚æµ‹ä¿¡æ¯ï¼‰ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­å¤„ç†æœ‰æ—¶é—´ä¾èµ–çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä½¿ç”¨ LSTMã€RNNã€æ³¨æ„åŠ›ç­‰ç»“æ„ã€‚
        # ä¸è¶³ history_len æ—¶ï¼Œç”¨å½“å‰æœ€æ–°è§‚æµ‹å¡«å……
        if len(self.obs_history) == 0:
            current_obs = self._get_obs()
            for _ in range(self.history_len):
                self.obs_history.append(current_obs)
        elif len(self.obs_history) < self.history_len:
            last_obs = self.obs_history[-1]
            for _ in range(self.history_len - len(self.obs_history)):
                self.obs_history.append(last_obs)
        return np.stack(self.obs_history, axis=0)

    def step(self, action):   # æ‰§è¡Œä¸€æ­¥åŠ¨ä½œï¼Œæ›´æ–°ç¯å¢ƒçŠ¶æ€ï¼Œè®¡ç®—å¥–åŠ±ï¼Œåˆ¤æ–­æ˜¯å¦ç»ˆæ­¢ï¼Œå¹¶è¿”å›æ–°çš„è§‚æµ‹åºåˆ—ã€å¥–åŠ±ã€doneæ ‡å¿—å’Œ info
        # åŠ¨ä½œæ‰§è¡Œï¼šæ˜ å°„å¹¶è£å‰ª
        acc = action[0] * 5.0     #action æ˜¯ SAC ç­–ç•¥è¾“å‡ºçš„äºŒç»´å‘é‡ [-1, 1] èŒƒå›´çš„å€¼ã€‚    æ›´æ–°é€Ÿåº¦ï¼Œå¹¶è£å‰ªåœ¨è®¾å®šèŒƒå›´å†…ã€‚
        steer = action[1] * 3.1416
        self.velocity += acc * self.env.step_time
        self.steering_angle += steer * self.env.step_time
        self.velocity = np.clip(self.velocity, -2, 2)
        self.steering_angle = np.clip(self.steering_angle, -0.523, 0.523)

        #ä¸ä»¿çœŸç¯å¢ƒäº¤äº’     å‘åº•å±‚ä»¿çœŸç¯å¢ƒï¼ˆirsimï¼‰ä¼ å…¥å®é™…åŠ¨ä½œï¼šçº¿é€Ÿåº¦ + è½¬è§’ã€‚
        self.env.step(np.array([[self.velocity], [self.steering_angle]]), 0)

        # æ”¶é›†å½“å‰è§‚æµ‹ & åŠ å…¥å†å²é˜Ÿåˆ—
        obs = self._get_obs()            #è·å–æ–°çš„è§‚æµ‹å¸§ï¼ˆæ¿€å…‰ + ç›®æ ‡ + è‡ªè½¦çŠ¶æ€ï¼‰å¹¶åŠ å…¥å†å²è§‚æµ‹åºåˆ—ã€‚
        self.obs_history.append(obs)     #éšç€æ—¶é—´æ¨ç§»ï¼Œobs_history ä¼šæ»šåŠ¨ç»´æŠ¤æœ€è¿‘ 5 å¸§è§‚æµ‹ã€‚

        # åˆ¤æ–­æ˜¯å¦ episode ç»“æŸ
        done = self.env.done()
        self.step_count += 1
        self.goal_count += 1

        # è·ç¦»è®¡ç®—
        [x_rel, y_rel, _] = self._target_relative_pose()
        current_distance = np.hypot(x_rel, y_rel)

        if self.prev_distance is None:
            self.prev_distance = current_distance

        #éšœç¢ç‰©æƒ©ç½š
        reward_obstacle = 0
        if min(np.array(self.env.get_lidar_scan()["ranges"], dtype=np.float32)) < 0.7:
            reward_obstacle = -0.5
            if min(np.array(self.env.get_lidar_scan()["ranges"], dtype=np.float32))-self.prev_min_distance<0:
                reward_obstacle += -0.5
        
        #è·ç¦»å¥–åŠ±
        delta_d = self.prev_distance - current_distance
        reward_distance = delta_d * 0.5
        
        #é™æ­¢æƒ©ç½š
        reward_movement = -0.2 if abs(self.velocity) < 0.3 else 0.0

        # åˆ¤æ–­å½“å‰ episode æ˜¯å¦å› ä¸ºåˆ°è¾¾ç›®æ ‡ã€ç¢°æ’æˆ–è¶…æ—¶è€Œç»“æŸ       
        reached_goal = current_distance <= 0.5
        collided = self.env.done() and not reached_goal
        goal_timeout = self.goal_count >= self.max_goal_steps

        #å®Œæˆç›®æ ‡ / ç¢°æ’ / è¶…æ—¶å¤„ç†
        if collided:
            reward_done = -15.0
            done = True
            print("episode done: collision")
        elif goal_timeout:
            reward_done = -15
            done = True
            print("episode done: timeout")
        elif reached_goal:
            reward_done = 15.0
            done = True
            self.goal_count = 0
            print("goal reached")

            new_goal = self.sample_goal()
            self.env.robot.set_goal(new_goal, init=False)
        else:
            reward_done = 0.0
            done = False

        #æœ€ç»ˆæ€»å¥–åŠ±è®¡ç®—    â†’ å‘ç›®æ ‡é è¿‘çš„å¥–åŠ± â†’ é™æ­¢çš„æƒ©ç½š â†’ æ¥è¿‘éšœç¢çš„æƒ©ç½š â†’ è¾¾æ ‡/å¤±è´¥ç»ˆæ­¢çš„å¥–åŠ±
        reward = reward_distance + reward_movement + reward_obstacle + reward_done

        self.prev_distance = current_distance    #æ›´æ–°ä¸Šä¸€æ—¶åˆ»çš„è·ç¦»ï¼ˆç”¨äºä¸‹æ¬¡è®¡ç®—è·ç¦»å˜åŒ–ï¼‰ï¼š
        self.prev_min_distance = min(np.array(self.env.get_lidar_scan()["ranges"], dtype=np.float32))    #è®°å½•å½“å‰æ—¶åˆ»æœ€å°çš„æ¿€å…‰è·ç¦»ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦è¶Šæ¥è¶Šé è¿‘éšœç¢ç‰©ï¼‰ï¼š
        # print(reward)

        self.env.render()    #å¯è§†åŒ–å½“å‰ç¯å¢ƒçŠ¶æ€ï¼šä¼šè®©ç¯å¢ƒæ˜¾ç¤ºç”»é¢ï¼Œå¸¸ç”¨äºè°ƒè¯•/è®­ç»ƒç›‘æ§ã€‚

        return self._get_obs_sequence(), reward, done, {}   # è¿”å› step() çš„ç»“æœï¼ˆç¬¦åˆ Gym æ¥å£æ ‡å‡†ï¼‰ï¼š

    def reset(self):
        #  æ¯å½“ä¸€ä¸ª episode ç»“æŸï¼ˆä¾‹å¦‚ç¢°æ’ã€åˆ°è¾¾ç›®æ ‡ã€è¶…æ—¶ï¼‰ï¼Œç®—æ³•ä¼šè‡ªåŠ¨è°ƒç”¨ env.reset() æ¥å¼€å§‹ä¸‹ä¸€è½®ã€‚è¿™æ®µä»£ç å®Œæˆäº†ä»¥ä¸‹åˆå§‹åŒ–å·¥ä½œï¼š   
        self.env.reset()  #é‡ç½®åº•å±‚ä»¿çœŸç¯å¢ƒï¼ˆirsimï¼‰ï¼Œæ¸…ç©ºçŠ¶æ€ã€æ—¶é—´æ­¥ç­‰ã€‚
         #ä¸ºéšœç¢ç‰© éšæœºé‡æ–°ç”Ÿæˆä½ç½®ï¼Œé¿å…æ¯æ¬¡éƒ½æ˜¯åŒä¸€ä¸ªç¯å¢ƒå¸ƒå±€ï¼›
        self.env.random_obstacle_position(range_low= [2, 2.5, -3.14], range_high= [13, 11.5, 3.14], ids= [6,7,8,9,10,11,12,13,14,15], non_overlapping = True)  
        #æ¸…ç©ºä¸åˆå§‹åŒ–çŠ¶æ€å˜é‡ï¼š
        self.velocity = 0.0
        self.steering_angle = 0.0
        self.prev_distance = None
        self.step_count = 0
        self.goal_count = 0
        #è®¾ç½®æœºå™¨äººåˆå§‹ä½ç½®ï¼š
        x = np.random.uniform(2, 12)
        y = np.random.uniform(0.7, 1.3)
        theta = np.random.uniform(0, np.pi)
        self.env.robot.set_state([x,y,theta,0])
        #è®¾ç½®æ–°çš„ç›®æ ‡ç‚¹
        goal = self.sample_goal()
        self.env.robot.set_goal(goal, init=False)
        #åˆå§‹åŒ–è§‚æµ‹åºåˆ—ï¼š
        obs = self._get_obs()
        self.obs_history.clear()
        for _ in range(self.history_len):
            self.obs_history.append(obs)
        #è¿”å› (5, obs_dim) çš„è§‚æµ‹åºåˆ—ï¼Œä½œä¸ºå½“å‰ episode çš„åˆå§‹è¾“å…¥ã€‚
        return self._get_obs_sequence()

    def close(self):   #å…³é—­åº•å±‚ä»¿çœŸå™¨ï¼Œé‡Šæ”¾èµ„æºï¼›
        self.env.end()

class LaserGoalFeatureExtractor(BaseFeaturesExtractor):       #ç»§æ‰¿è‡ª SB3 çš„ BaseFeaturesExtractorï¼Œæ˜¯è‡ªå®šä¹‰ç‰¹å¾æå–å™¨çš„æ ‡å‡†æ–¹å¼ã€‚
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):
        super().__init__(observation_space, features_dim)

        self.laser_dim = 180    # æ¯å¸§æ¿€å…‰è§‚æµ‹ç»´åº¦
        self.goal_dim = 5       # æ¯å¸§ç›®æ ‡ç›¸å…³ä¿¡æ¯ï¼ˆç›®æ ‡ä½ç½® + é€Ÿåº¦ + æœå‘ç­‰ï¼‰
        self.seq_len = 5        # æ—¶åºé•¿åº¦ï¼ˆè§‚æµ‹å†å²å¸§æ•°ï¼‰

##????????????????????????????????????????????????????????????????????
        # CNN for laser frames (each frame: 1 channel x 180)   # å·ç§¯æå–æ¿€å…‰ç‰¹å¾   
        # åœ¨ PyTorch çš„ nn.Conv1d ä¸­ï¼Œè¾“å…¥å¼ é‡æ ¼å¼ä¸º [batch_size, channels, length]ï¼š
        # channelsï¼šé€šé“æ•°ï¼ˆç›¸å½“äºæ¯ä¸ªâ€œä¼ æ„Ÿå™¨â€ä¸€æ¡çº¿ï¼‰ï¼›
        # lengthï¼šä¸€æ¡é€šé“çš„åºåˆ—é•¿åº¦ï¼ˆå¦‚æ¿€å…‰é›·è¾¾çš„ä¸€å¸§æ˜¯180ç»´ï¼‰
        self.conv1 = nn.Conv1d(in_channels=5, out_channels=32, kernel_size=5, stride=2)    #å¯¹åº”çš„æ˜¯ç‰¹å¾ï¼ˆå¯¹åº”å‰å5å¸§  è¿›5å‡º32  å·ç§¯æ˜¯çš„å°èŒƒå›´æ˜¯5ï¼‰
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=5, kernel_size=3, stride=2)    #æ¥ä¸Šä¸€è¡Œï¼ˆå¯¹åº”ä¸Šé¢çš„å‡ºçš„32  è¿›32å‡º5  å·ç§¯æ˜¯çš„å°èŒƒå›´æ˜¯3ï¼‰
 
        # Calculate CNN output dimension using correct dummy input    è®¡ç®—å·ç§¯è¾“å‡ºç»´åº¦
        with torch.no_grad():
            dummy = torch.zeros(1, 5, self.laser_dim)  # [1, 1, 180]      #ï¼ˆ180 è¿›å…¥2ä¸ªå·ç§¯å±‚ï¼‰
            dummy = self.conv1(dummy)
            dummy = self.conv2(dummy)
            self.cnn_feat_dim = dummy.shape[2]    ## å±•å¹³åä¸ºå…¨è¿æ¥å±‚è¾“å…¥

        # FC for goal frames       æŠŠæ¯å¸§ 5 ç»´çš„ç›®æ ‡çŠ¶æ€å‹ç¼©æˆ 32 ç»´ï¼›   åé¢é€å…¥ goal LSTM ç¼–ç æ—¶åºä¿¡æ¯ã€‚
        self.goal_fc = nn.Linear(self.goal_dim, 32)         # ï¼ˆ5ä¸ªï¼ˆç›®æ ‡ä½ç½® + é€Ÿåº¦ + æœå‘ç­‰ï¼‰   è¿›å…¥å…¨è¿æ¥å±‚ï¼‰
  
        # LSTMs  æ—¶åºå»ºæ¨¡ï¼šLSTM ç¼–ç å™¨   æŠŠå·ç§¯åçš„æ¿€å…‰ç‰¹å¾åºåˆ— [B, 5, cnn_feat_dim] è¾“å…¥ LSTMï¼Œæå–æ—¶åºä¾èµ–ï¼›  åŒç†ï¼Œç›®æ ‡çŠ¶æ€åºåˆ—ä¹Ÿè¾“å…¥ä¸€ä¸ªå°çš„ LSTMã€‚     ä¸¤ä¸ªéƒ½è¿‡LSTM
        self.laser_lstm = nn.LSTM(input_size=self.cnn_feat_dim, hidden_size=128, batch_first=True)
        self.goal_lstm = nn.LSTM(input_size=32, hidden_size=32, batch_first=True)

        self.multihead_attn = nn.MultiheadAttention(embed_dim=128+32, num_heads=5, batch_first=True)   #å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èåˆ  å¯¹æ¿€å…‰ + ç›®æ ‡çš„èåˆç‰¹å¾è¿›è¡Œ attention äº¤äº’ï¼›   ä¸Šè¿°æ•°æ®èåˆ 
        # Final projection  æœ€ç»ˆè¾“å‡ºå±‚  å°†æ³¨æ„åŠ›è¾“å‡ºçš„æœ€åä¸€æ­¥æˆ–è€…èšåˆç‰¹å¾æ˜ å°„ä¸ºæŒ‡å®šç»´åº¦ï¼ˆå¦‚ 128ï¼‰ï¼›  ç»™ç­–ç•¥/å€¼ç½‘ç»œä½¿ç”¨ã€‚
        self.fc = nn.Linear(128 + 32, features_dim)

    def forward(self, observations: torch.Tensor) -> torch.Tensor:    #æŠŠç¯å¢ƒè§‚æµ‹è¾“å…¥ï¼Œç»è¿‡ç‰¹å¾æå–å’Œèåˆï¼Œè¾“å‡ºç”¨äºç­–ç•¥æˆ–ä»·å€¼ä¼°è®¡çš„ç‰¹å¾
        
       
        B = observations.size(0)     #å– batch å¤§å° Bã€‚

        # å°†è§‚æµ‹æ‹†åˆ†ä¸ºæ¿€å…‰é›·è¾¾åºåˆ—å’Œç›®æ ‡çŠ¶æ€åºåˆ—ã€‚
        laser_seq = observations[:, :, :self.laser_dim]  # [B, 5, 180]
        goal_seq = observations[:, :, self.laser_dim:]  # [B, 5, 5]

        # æ¿€å…‰æ•°æ®å…ˆç»è¿‡ä¸¤ä¸ª Conv1d å·ç§¯å±‚æå–ç©ºé—´ç‰¹å¾ï¼Œå†é€šè¿‡ LSTM æ•æ‰æ—¶é—´åºåˆ—ä¾èµ–ï¼Œå¾—åˆ°æ¿€å…‰åºåˆ—ç‰¹å¾ã€‚
        x = F.relu(self.conv1(laser_seq))
        x = F.relu(self.conv2(x))
        laser_h, _ = self.laser_lstm(x)  # laser_hn: [1, B, 128]
        
        #ç›®æ ‡çŠ¶æ€æ•°æ®å…ˆç”¨å…¨è¿æ¥å±‚é™ç»´ï¼Œå†ç”¨ LSTM æå–æ—¶åºä¿¡æ¯ã€‚
        g = F.relu(self.goal_fc(goal_seq))  # [B*5, 32]\
        g_h, _ = self.goal_lstm(g)  # goal_hn: [1, B, 32]

        #å°†æ¿€å…‰å’Œç›®æ ‡åºåˆ—çš„æ—¶åºç‰¹å¾æ‹¼æ¥èåˆã€‚
        features = torch.cat((laser_h, g_h), dim=2)

        #é€šè¿‡å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMultihead Attentionï¼‰è®©æ¿€å…‰å’Œç›®æ ‡ç‰¹å¾äº’ç›¸äº¤äº’ï¼Œå¢å¼ºèåˆæ•ˆæœã€‚
        attn_output, attn_weights = self.multihead_attn(features, features, features)

        # last = attn_output[:, -1, :]  å¯¹åºåˆ—çš„æ¯ä¸ªæ—¶é—´æ­¥ç‰¹å¾å–å‡å€¼ï¼Œå¾—åˆ°ä¸€ç»„èåˆç‰¹å¾ï¼Œå†ç»è¿‡çº¿æ€§å˜æ¢+ReLUæ¿€æ´»ï¼Œè¾“å‡ºè¯¥æ—¶åˆ»çš„ç‰¹å¾å‘é‡ï¼ˆç”¨äºåç»­ç­–ç•¥æˆ–ä»·å€¼ç½‘ç»œï¼‰ã€‚
        pool = attn_output.mean(dim=1)
        action = F.relu(self.fc(pool))
        return action

    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:
        return self.policy_net(features)

    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:
        return self.value_net(features)


if __name__ == "__main__":
    # åˆ›å»ºç¯å¢ƒ
    env = IRSIMEnv('env/moving.yaml', display=True)

    # æ¨¡å‹ä¿å­˜ç›®å½•
    log_dir = "./sequence_moving_models"     #åˆ›å»ºä¸€ä¸ªç›®å½• ./sequence_moving_models ç”¨äºä¿å­˜æ¨¡å‹ã€æ—¥å¿—å’Œä¸­é—´ç»“æœã€‚
    os.makedirs(log_dir, exist_ok=True)      #exist_ok=True è¡¨ç¤ºï¼šå¦‚æœç›®å½•å·²å­˜åœ¨å°±ä¸ä¼šæŠ¥é”™ã€‚   

    #è¯„ä¼°å›è°ƒå‡½æ•°ï¼ˆEvalCallbackï¼‰   è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®šæœŸï¼ˆæ¯ eval_freq æ­¥ï¼‰åœ¨ç¯å¢ƒä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°ï¼› å¦‚æœå‘ç°æ–°çš„æ›´ä¼˜æ¨¡å‹ï¼ˆä¾‹å¦‚ reward æ›´é«˜ï¼‰ï¼Œå°±ä¼šä¿å­˜åˆ° best_model_save_path æŒ‡å®šçš„ç›®å½•ï¼›
    callback_save_best_model = EvalCallback(
        env,                          # è¯„ä¼°ç¯å¢ƒ
        best_model_save_path=log_dir,  # ä¿å­˜â€œå½“å‰æœ€ä¼˜æ¨¡å‹â€çš„è·¯å¾„
        log_path=log_dir,              # è¯„ä¼°æŒ‡æ ‡æ—¥å¿—ä¿å­˜è·¯å¾„
        eval_freq=4096,                # æ¯éš”å¤šå°‘æ­¥è¯„ä¼°ä¸€æ¬¡
        deterministic=True,           # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆé€‚ç”¨äºæµ‹è¯•ï¼‰
        render=False                  # æ˜¯å¦åœ¨è¯„ä¼°æ—¶å¯è§†åŒ–æ¸²æŸ“
    )   
    
    # å›è°ƒå‡½æ•°å°±æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„â€œè‡ªåŠ¨ç›‘è§†å™¨â€ï¼Œåœ¨å…³é”®èŠ‚ç‚¹è‡ªåŠ¨æ‰§è¡ŒæŸäº›ä»»åŠ¡ï¼ˆä¿å­˜æ¨¡å‹ã€è¯„ä¼°æ€§èƒ½ã€æå‰åœæ­¢ç­‰ï¼‰
    callback_list = CallbackList([callback_save_best_model])       #æŠŠå¤šä¸ªåŠŸèƒ½æ•´åˆåˆ°ä¸€èµ·ï¼Œåªéœ€æŠŠè¿™ä¸ª callback_list ä¼ ç»™ .learn() å³å¯
    
    #ç­–ç•¥ç½‘ç»œå‚æ•° policy_kwargs     #è¿™ä¸ªé…ç½®ä¼šä¼ å…¥ä¾‹å¦‚ PPO æˆ– SAC çš„æ„é€ å‡½æ•°ä¸­ï¼Œæ¥æ§åˆ¶ç¥ç»ç½‘ç»œç»“æ„å’Œç‰¹å¾æå–å™¨ã€‚
    policy_kwargs = dict( 
        features_extractor_class=LaserGoalFeatureExtractor,    #è‡ªå®šä¹‰çš„ç‰¹å¾æå–å™¨ç±»ï¼Œä¾‹å¦‚ç”¨äºå¤„ç†æ¿€å…‰é›·è¾¾+ç›®æ ‡ä½ç½®ç­‰ç»„åˆè¾“å…¥
        features_extractor_kwargs=dict(features_dim=256),      #æå–å™¨çš„å‚æ•°ï¼Œè¿™é‡ŒæŒ‡å®šè¾“å‡ºç‰¹å¾ç»´åº¦ä¸º 256
        net_arch=dict(pi=[256,128,64], qf=[256,128,64])        #ç­–ç•¥ç½‘ç»œ(pi ç”¨äºè¾“å‡ºåŠ¨ä½œçš„éšè—å±‚ç»´åº¦)å’Œä»·å€¼ç½‘ç»œ(qf ç”¨äºä¼°è®¡Qå€¼çš„éšè—å±‚ç»´åº¦)   3 å±‚å…¨è¿æ¥éšè—å±‚ï¼Œæ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°ä¾æ¬¡ä¸ºï¼š256 128 64
    )

    # åˆå§‹åŒ–æ¨¡å‹
    model = SAC(        #è™½ç„¶æ˜¾ç¤ºçš„æ˜¯å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥ï¼ˆMLP Policyï¼‰ï¼Œä½¿ç”¨ Soft Actor-Critic ç®—æ³•ï¼Œæ˜¯ä¸€ç§åŸºäºå€¼å‡½æ•°çš„ç¦»ç­–ç•¥æ–¹æ³•ï¼Œå…·æœ‰é«˜æ ·æœ¬æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œé€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´ä»»åŠ¡ï¼Œå¦‚æœºå™¨äººæ§åˆ¶ã€æ— äººé©¾é©¶ç­‰ã€‚
        policy="MlpPolicy",      #å®é™…ä¸Šç”¨çš„æ˜¯ä½ è‡ªå®šä¹‰çš„ LaserGoalFeatureExtractorï¼Œè™½ç„¶å¤–å±‚å†™ç€ MlpPolicyï¼Œä½†æœ¬è´¨ä¼šåŠ è½½ä½ æŒ‡å®šçš„ç½‘ç»œç»“æ„
        env=env,
        policy_kwargs=policy_kwargs,    #ç”¨äºä¼ å…¥è‡ªå®šä¹‰ç­–ç•¥ç»“æ„çš„é…ç½®å‚æ•°ï¼Œä½ ä¹‹å‰å®šä¹‰çš„æ˜¯ï¼š
        verbose=1,                     #æ§åˆ¶æ§åˆ¶å°æ—¥å¿—çš„è¾“å‡ºç­‰çº§ï¼› 0 è¡¨ç¤ºé™é»˜ï¼Œ1 è¡¨ç¤ºæ¯æ­¥è®­ç»ƒéƒ½ä¼šæœ‰æ‘˜è¦è¾“å‡ºï¼Œ2 æ˜¯æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ã€‚                                                           
        learning_rate=1e-4,              # å­¦ä¹ ç‡ï¼Œæ§åˆ¶ç­–ç•¥å’Œä»·å€¼ç½‘ç»œçš„æ¢¯åº¦æ›´æ–°æ­¥é•¿ï¼› å¯¹ SAC æ¥è¯´ï¼Œé€šå¸¸è®¾ç½®åœ¨ 3e-4 åˆ° 1e-4 éƒ½æ˜¯åˆç†çš„ï¼Œä½ è®¾ç½®å¾—æ¯”è¾ƒä¿å®ˆ
        tensorboard_log="./sac_laser_goal_tensorboard/"   #è¡¨ç¤ºæŠŠè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—è¾“å‡ºåˆ°è¯¥ç›®å½•ï¼›
    )


    # model = SAC.load("sequence_moving_models/akm_best_model", env=env)
    # model.learn(total_timesteps=1000000, callback=callback_list, progress_bar=True)     #  è¡¨ç¤ºè®­ç»ƒæ€»æ­¥æ•°ä¸º 100 ä¸‡æ­¥ï¼›  callback_list å›è°ƒå‡½æ•°åˆ—è¡¨   ä¼šåœ¨è®­ç»ƒæ—¶åœ¨æ§åˆ¶å°æ˜¾ç¤ºä¸€ä¸ª tqdm çš„è¿›åº¦æ¡ï¼›
    model = SAC.load("sequence_moving_models/new_best_model", env=env)                    #  ä»è·¯å¾„ sequence_moving_models/new_best_model åŠ è½½ä¸€ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼›
    # model.learn(total_timesteps=1000000, callback=callback_list, progress_bar=True)
    # è¯„ä¼°
    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50)   #è¿™è¡Œæ˜¯ç”¨ SB3 è‡ªå¸¦çš„è¯„ä¼°å·¥å…· evaluate_policy(...) è¿›è¡Œè¯„ä¼°ï¼Œn_eval_episodes=50ï¼šåœ¨ 50 ä¸ªç‹¬ç«‹ episode ä¸Šæµ‹è¯•æ¨¡å‹è¡¨ç°
    print(f"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}")

    #  æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š
    #  å¾ªç¯ç›´åˆ°è¾¾åˆ° total_timesteps:
    # - æ‰§è¡ŒåŠ¨ä½œ
    # - ä¸ç¯å¢ƒäº¤äº’è·å–åé¦ˆï¼ˆobs, reward, doneï¼‰
    # - å­˜å‚¨ç»éªŒ
    # - è¾¾åˆ°ä¸€å®šé—´éš”åï¼Œä»ç»éªŒä¸­é‡‡æ ·ï¼Œè¿›è¡Œç­–ç•¥ç½‘ç»œä¸Qç½‘ç»œæ›´æ–°
    # - æ¯éš” eval_freq æ­¥ï¼Œè¿›è¡Œä¸€æ¬¡å›è°ƒï¼ˆè¯„ä¼°ã€ä¿å­˜ç­‰ï¼‰
    # - è¾“å‡º tensorboard æ—¥å¿—å’Œæ§åˆ¶å°è¿›åº¦
